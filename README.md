# LLM-Answer-Scoring-Hallucination-Detection-Engine
Truth-aware LLM answer scoring engine with hallucination detection and factual verification.

# ğŸ§  Hallucination-Aware Multi-LLM Analyzer

A Flask-based system that queries multiple Large Language Models (LLMs) in parallel and evaluates their responses using semantic similarity, factual verification, and overconfidence detection.

---

## ğŸš€ Features

- Parallel querying of multiple Ollama-hosted LLMs
- Hallucination-aware scoring system
- Overconfidence penalty based on language usage
- Semantic similarity validation
- Multithreaded execution for faster responses
- Simple web-based interface

---

## ğŸ“ Project Structure

hallucination-model/
â”œâ”€â”€ app.py
â”œâ”€â”€ agents.py
â”œâ”€â”€ scoring.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ templates/
â””â”€â”€ index.html

---

## âš™ï¸ Installation & Setup

Follow the steps below in order.

---

### 1ï¸âƒ£ Prerequisites

- Python 3.9 or higher
- Git
- Ollama installed and running

Start Ollama:

ollama serve

2ï¸âƒ£ Clone the Repository

git clone https://github.com/YOUR_USERNAME/hallucination-model.git
cd hallucination-model

3ï¸âƒ£ Create and Activate Virtual Environment (Recommended)

python -m venv .venv

Windows
.venv\Scripts\activate

Linux / macOS
source .venv/bin/activate

4ï¸âƒ£ Install Python Dependencies

pip install -r requirements.txt
python -m spacy download en_core_web_sm

5ï¸âƒ£ Download Required Models

ollama pull phi3
ollama pull qwen2.5:1.5b
ollama pull deepseek-r1:1.5b

6ï¸âƒ£ Run the Application

python app.py
Open your browser and visit:
http://127.0.0.1:5000

##ğŸ§ª How to Use

Enter a factual query (e.g. Who invented the incandescent light bulb?)

Click Analyze

The system will:

Query multiple LLMs in parallel

Apply hallucination and confidence penalties

Rank responses

Highlight the most reliable answer

##ğŸ§  Why This Project?

Large Language Models often:

Hallucinate facts

Invent sources

Sound confident even when incorrect

This project evaluates truth-likelihood, not fluency.

##ğŸ”® Future Improvements

Dynamic knowledge base integration

Topic-agnostic scoring

Model consensus scoring

Explainable score breakdown

Research-grade evaluation metrics

